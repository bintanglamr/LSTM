# -*- coding: utf-8 -*-
"""lstm-multivariate-forecasting [GHI].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1haFvyuwaPwOVsbH38ii4oU_FvVTLfDIL

## Import Packages
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
from math import sqrt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from keras.models import Sequential, load_model
from keras.layers import Dense, LSTM, Dropout, BatchNormalization
from sklearn.metrics import mean_squared_error as mse
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""## Load Data"""

df_train = pd.read_csv('/content/busan_dataset.csv')
df_train

df_train.info()

df_test = pd.read_csv("/content/busan_dataset.csv")
df_test

# Checking null values
print(df_train.isnull().sum() , "\n---------------- \n" , df_test.isnull().sum() )

df_train.describe()

"""## Data vislualization and Feature scaling"""



df_train_scaled = df_train.copy()
df_test_scaled = df_test.copy()

# Define the mapping dictionary
mapping = {'NE': 0, 'SE': 1, 'NW': 2, 'cv': 3}

# Replace the string values with numerical values
df_train_scaled['wnd_dir'] = df_train_scaled['wnd_dir'].map(mapping)
df_test_scaled['wnd_dir'] = df_test_scaled['wnd_dir'].map(mapping)

df_train_scaled['date'] = pd.to_datetime(df_train_scaled['date'])
# Resetting the index
df_train_scaled.set_index('date', inplace=True)
df_train_scaled.head()

df_train_scaled = df_train.copy()
df_test_scaled = df_test.copy()

df_train_scaled['Date'] = pd.to_datetime(df_train_scaled['Date'])

# Resetting the index
df_train_scaled.set_index('Date', inplace=True)
df_train_scaled.head()

required_cols = ['GHI_Average', 'SunZenith_KMU', 'Ambient_Pressure', 'CI_Beyer']
df_train_scaled = df_train_scaled[required_cols]
df_test_scaled = df_test_scaled[required_cols]

df_train_scaled.head()

# Create subplots
fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 15))

# Define a color palette
palette = sns.color_palette("deep", n_colors=4)

# Plot GHI_Average
axes[0].plot(df_train['GHI_Average'], color=palette[0])
axes[0].set_title('GHI Average')
axes[0].set_xlabel('Date')
axes[0].set_ylabel('Value')

# Plot SunZenith_KMU
axes[1].plot(df_train['SunZenith_KMU'], color=palette[1])
axes[1].set_title('SunZenith KMU')
axes[1].set_xlabel('Date')
axes[1].set_ylabel('Value')

# Plot Ambient_Pressure
axes[2].plot(df_train['Ambient_Pressure'], color=palette[2])
axes[2].set_title('Ambient Pressure')
axes[2].set_xlabel('Date')
axes[2].set_ylabel('Value')

# Plot CI_Beyer
axes[3].plot(df_train['CI_Beyer'], color=palette[3])
axes[3].set_title('Cloud Index')
axes[3].set_xlabel('Date')
axes[3].set_ylabel('Value')

# Adjust layout to prevent overlap
plt.tight_layout()

# Show the plots
plt.show()

sns.set(style="darkgrid")

fig, axs = plt.subplots(2,2, figsize=(24,14))

sns.histplot(data=df_test_scaled, x="GHI_Average", kde=True, color="skyblue", ax=axs[0, 0])
sns.histplot(data=df_test_scaled, x="SunZenith_KMU", kde=True, color="olive", ax=axs[0, 1])
sns.histplot(data=df_test_scaled, x="Ambient_Pressure", kde=True, color="gold", ax=axs[1, 0])
sns.histplot(data=df_test_scaled, x="CI_Beyer", kde=True, color="teal", ax=axs[1, 1])

plt.show()

scaler = MinMaxScaler()

# Define the columns to scale
columns = (['GHI_Average', 'SunZenith_KMU', 'Ambient_Pressure', 'CI_Beyer'])

df_test_scaled = df_test_scaled[columns]

# Scale the selected columns to the range 0-1
df_train_scaled[columns] = scaler.fit_transform(df_train_scaled[columns])
df_test_scaled[columns] = scaler.transform(df_test_scaled[columns])

# Show the scaled data
df_train_scaled.head()

df_test_scaled.head()

"""## Split the data into training and test sets"""

df_train_scaled = np.array(df_train_scaled)
df_test_scaled = np.array(df_test_scaled)

X = []
y = []
n_future = 1
n_past = 6

#  Train Sets
for i in range(n_past, len(df_train_scaled) - n_future+1):
    X.append(df_train_scaled[i - n_past:i, 1:df_train_scaled.shape[1]])
    y.append(df_train_scaled[i + n_future - 1:i + n_future, 0])
X_train, y_train = np.array(X), np.array(y)

#  Test Sets

X = []
y = []
for i in range(n_past, len(df_test_scaled) - n_future+1):
    X.append(df_test_scaled[i - n_past:i, 1:df_test_scaled.shape[1]])
    y.append(df_test_scaled[i + n_future - 1:i + n_future, 0])
X_test, y_test = np.array(X), np.array(y)

print('X_train shape : {}   y_train shape : {} \n'
      'X_test shape : {}      y_test shape : {} '.format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))

"""## Create a LSTM model"""

# design network

model = Sequential()
model.add(LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(16, return_sequences=False))
model.add(Dense(y_train.shape[1]))

# Compile the model
model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=[RootMeanSquaredError()])

# Define callbacks for avoiding overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
checkpoint = ModelCheckpoint('/content/best_model.h5.keras', monitor='val_loss', save_best_only=True)

model.summary()

# fit network
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping, checkpoint], shuffle=False)

"""## Evaluate the Model"""

# Load the best model
best_model = load_model('best_model.h5.keras')

plt.figure(figsize=(15,6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

test_predictions = best_model.predict(X_test).flatten()
test_results = pd.DataFrame(data={'Train Predictions': test_predictions,
                                  'Actual':y_test.flatten()})
test_results.head()

plt.plot(test_results['Train Predictions'][:100], color = 'orchid', label='Predicted GHI')
plt.plot(test_results['Actual'][:100], color = 'teal', label='Actual GHI')
plt.title('Actual vs Predicted GHI')
plt.xlabel('Time')
plt.ylabel('GHI')
plt.legend()
plt.show()

rmse = sqrt(mse(y_test, test_predictions))
print('Test RMSE: %.5f' % rmse)

# Make predictions
y_pred_scaled = model.predict(X_test)

# Since only the 'pollution' column was used as the output,
# create an empty array with the same shape as the original dataset
# but only fill the pollution column with predictions
y_pred_actual = np.zeros((y_pred_scaled.shape[0], df_test_scaled.shape[1]))
y_pred_actual[:, 0] = y_pred_scaled[:, 0]  # fill only the pollution column

# Inverse transform the predictions
y_pred_actual = scaler.inverse_transform(y_pred_actual)

# Get the actual pollution values
y_pred_actual = y_pred_actual[:, 0]  # extract the pollution column

# Create an empty array to store the actual y_test values
y_test_actual = np.zeros((y_test.shape[0], df_test_scaled.shape[1]))
y_test_actual[:, 0] = y_test[:, 0]

# Inverse transform the y_test values
y_test_actual = scaler.inverse_transform(y_test_actual)

# Get the actual pollution values
y_test_actual = y_test_actual[:, 0]  # extract the pollution column

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(y_test_actual[:200], label='True Values')
plt.plot(y_pred_actual[:200], label='Predicted Values', linestyle='--')
plt.title('Actual vs Predicted GHI')
plt.ylabel('GHI')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
import numpy as np

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_actual))

print('Test RMSE: %.3f' % rmse)

import pandas as pd

# Assuming df_test has a 'Date' column
date_column = df_test['Date'].iloc[n_past:len(df_test_scaled) - n_future + 1]

# Create a DataFrame for the results
results_df = pd.DataFrame({
    'Date': date_column,
    'Actual Pollution': y_test_actual,
    'Predicted Pollution': y_pred_actual
})

# Display the first few rows of the table
results_df.head()

# Save the table to a CSV file
results_df.to_csv('pollution_predictions.csv', index=False)

# Save the table to an Excel file
results_df.to_excel('pollution_predictions.xlsx', index=False)